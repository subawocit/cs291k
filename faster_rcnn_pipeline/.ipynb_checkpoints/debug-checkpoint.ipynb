{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707baf14-e857-4d04-9cc4-3ceb07c0fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/pipeline/training_pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd /hdd/yuchen/pipeline/training_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4349a269-0de9-4baa-91c4-a1c85d23ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils.engine import (\n",
    "    train_one_epoch, evaluate, utils\n",
    ")\n",
    "from torch.utils.data import (\n",
    "    distributed, RandomSampler, SequentialSampler\n",
    ")\n",
    "from datasets import (\n",
    "    create_train_dataset, create_valid_dataset, \n",
    "    create_train_loader, create_valid_loader\n",
    ")\n",
    "from models.create_fasterrcnn_model import create_model\n",
    "from utils.general import (\n",
    "    set_training_dir, Averager, \n",
    "    save_model, save_loss_plot,\n",
    "    show_tranformed_image,\n",
    "    save_mAP, save_model_state, SaveBestModel,\n",
    "    yaml_save, init_seeds\n",
    ")\n",
    "from utils.logging import (\n",
    "    set_log, coco_log,\n",
    "    set_summary_writer, \n",
    "    tensorboard_loss_log, \n",
    "    tensorboard_map_log,\n",
    "    csv_log,\n",
    "    wandb_log, \n",
    "    wandb_save_model,\n",
    "    wandb_init\n",
    ")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torchinfo\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68021864-9909-47e2-bd2a-df835d172f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n",
      "Checking Labels and images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897e0d0ae8694daeb4e062f7db90b5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Labels and images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0dedb7b10640b783d03244e0003c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "RANK = int(os.getenv('RANK', -1))\n",
    "\n",
    "# For same annotation colors each time.\n",
    "np.random.seed(42)\n",
    "\n",
    "    \n",
    "# Settings/parameters/constants.\n",
    "TRAIN_DIR_IMAGES = '/hdd/yuchen/pipeline/training_pipeline/data_paper/train'\n",
    "TRAIN_DIR_LABELS = '/hdd/yuchen/pipeline/training_pipeline/data_paper/train'\n",
    "VALID_DIR_IMAGES = '/hdd/yuchen/pipeline/training_pipeline/data_paper/valid'\n",
    "VALID_DIR_LABELS = '/hdd/yuchen/pipeline/training_pipeline/data_paper/valid'\n",
    "CLASSES = [ '__background__', 'landslide']\n",
    "NUM_CLASSES = 2\n",
    "NUM_WORKERS = 1\n",
    "DEVICE = 'cuda:0'\n",
    "print(\"device\",DEVICE)\n",
    "NUM_EPOCHS = 20\n",
    "SAVE_VALID_PREDICTIONS = True\n",
    "BATCH_SIZE = 2\n",
    " \n",
    "\n",
    "    # Model configurations\n",
    "IMAGE_SIZE = 512\n",
    "    \n",
    "train_dataset = create_train_dataset(\n",
    "        TRAIN_DIR_IMAGES, \n",
    "        TRAIN_DIR_LABELS,\n",
    "        IMAGE_SIZE, \n",
    "        CLASSES,\n",
    "        use_train_aug=False,\n",
    "        mosaic=0.0,\n",
    "        square_training=True\n",
    "    )\n",
    "valid_dataset = create_valid_dataset(\n",
    "        VALID_DIR_IMAGES, \n",
    "        VALID_DIR_LABELS, \n",
    "        IMAGE_SIZE, \n",
    "        CLASSES,\n",
    "        square_training=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc139a78-0568-4fbf-b790-580fdea0b129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1922, 0.1490, 0.1059,  ..., 0.1333, 0.1412, 0.1529],\n",
       "          [0.1569, 0.1333, 0.1059,  ..., 0.1412, 0.1373, 0.1333],\n",
       "          [0.1373, 0.1373, 0.1373,  ..., 0.1451, 0.1216, 0.1059],\n",
       "          ...,\n",
       "          [0.2941, 0.2980, 0.2863,  ..., 0.1725, 0.1765, 0.1647],\n",
       "          [0.2902, 0.2980, 0.2863,  ..., 0.1647, 0.1686, 0.1608],\n",
       "          [0.2824, 0.2941, 0.2824,  ..., 0.1765, 0.1882, 0.1882]],\n",
       " \n",
       "         [[0.1961, 0.1529, 0.1098,  ..., 0.1490, 0.1569, 0.1686],\n",
       "          [0.1608, 0.1373, 0.1098,  ..., 0.1569, 0.1529, 0.1490],\n",
       "          [0.1412, 0.1412, 0.1412,  ..., 0.1608, 0.1373, 0.1216],\n",
       "          ...,\n",
       "          [0.2784, 0.2824, 0.2706,  ..., 0.2000, 0.2039, 0.1922],\n",
       "          [0.2745, 0.2824, 0.2706,  ..., 0.1922, 0.1961, 0.1882],\n",
       "          [0.2667, 0.2784, 0.2667,  ..., 0.2039, 0.2157, 0.2157]],\n",
       " \n",
       "         [[0.2039, 0.1608, 0.1176,  ..., 0.1529, 0.1608, 0.1725],\n",
       "          [0.1686, 0.1451, 0.1176,  ..., 0.1608, 0.1569, 0.1529],\n",
       "          [0.1490, 0.1490, 0.1490,  ..., 0.1647, 0.1412, 0.1255],\n",
       "          ...,\n",
       "          [0.2314, 0.2353, 0.2235,  ..., 0.1725, 0.1725, 0.1608],\n",
       "          [0.2275, 0.2353, 0.2235,  ..., 0.1647, 0.1647, 0.1569],\n",
       "          [0.2196, 0.2314, 0.2196,  ..., 0.1765, 0.1843, 0.1843]]]),\n",
       " {'boxes': tensor([[150, 198, 232, 252]]),\n",
       "  'labels': tensor([1]),\n",
       "  'area': tensor([4428.]),\n",
       "  'iscrowd': tensor([0]),\n",
       "  'image_id': tensor([0])})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in valid_dataset:\n",
    "    break\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23e189-dc64-4ca1-8bed-0575b1997043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fd39c-bd14-442d-a343-54a614209651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e0439-c86e-4bf7-8126-f947dc1787c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850d499-7b07-43d9-add0-47bc15cdd510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
