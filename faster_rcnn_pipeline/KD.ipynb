{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc895db-d91c-4311-a59b-b9abc175149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTForImageClassification, SwinForImageClassification\n",
    "from transformers import AutoFeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98fd834a-2903-49b9-9a23-c5dfd16ef516",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/tasks/knowledge_distillation_for_image_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4143a6c9-eca6-4089-a06b-e5ddf8b6261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    batch_size = 16\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    device = 'cuda:1'\n",
    "    temperature = 5\n",
    "    lambda_param = 0.5\n",
    "\n",
    "args = Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb256cd8-7c88-4217-8d45-518a9c973b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        img_dir = '/hdd/yuchen/satdata/l4s_classification/'\n",
    "        img_lst = []\n",
    "        label_lst = []\n",
    "        \n",
    "        files = glob.glob(img_dir+'n0/*')\n",
    "        for img_idx in files:\n",
    "            img_lst.append(Image.open(img_idx).convert(\"RGB\"))\n",
    "            label_lst.append(0)\n",
    "        print(len(files))\n",
    "        files = glob.glob(img_dir+'n1/*')\n",
    "        print(len(files))\n",
    "        \n",
    "        for img_idx in files:\n",
    "            img_lst.append(Image.open(img_idx).convert(\"RGB\"))\n",
    "            label_lst.append(1)\n",
    "            \n",
    "        self.img_labels = label_lst\n",
    "        self.img = img_lst\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img[idx]\n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e838d3f-ac67-4d2b-89ab-2ad1c4fdd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to the input size expected by your model\n",
    "    transforms.ToTensor(),  # Convert images to Tensor\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize images\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f582cf81-8976-45d9-81a7-5308858a296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567\n",
      "2231\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset\n",
    "custom_dataset = CustomImageDataset(transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = args.batch_size\n",
    "shuffle = True\n",
    "\n",
    "# Split sizes\n",
    "total_size = len(custom_dataset)\n",
    "train_size = int(total_size * 0.8)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c023d29b-366f-42b9-85bc-7d102acb269c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "for batch in valid_loader:\n",
    "    ct += 1\n",
    "    if ct == 2: break\n",
    "\n",
    "print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156aeb1f-08ce-4d95-8e03-c6ab63608ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2, 3, figsize=(15, 10)) # Creating a 2x3 subplot\n",
    "\n",
    "# for i in range(6):\n",
    "#     img = batch[0][i].permute(1,2,0).detach().cpu().numpy()\n",
    "#     axs[i//3, i%3].imshow(img)\n",
    "#     axs[i//3, i%3].set_xticks([])\n",
    "#     axs[i//3, i%3].set_yticks([])\n",
    "\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f4f1f8-5991-4f47-be07-3426d6df8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('/hdd/yuchen/landslidesimg.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7fbf7-1c30-4662-9760-f698c810b1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc6b608-eaf6-47db-a4af-1c0ddc6ee78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = ViTForImageClassification.from_pretrained(\"google/vit-large-patch16-224\")\n",
    "teacher_model.classifier =  nn.Linear(in_features=1024, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c701b5-517f-4e4a-8f2d-92211081cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/hdd/yuchen/satdata/weights/fmow_pretrain.pth')['model']\n",
    "\n",
    "teacher_model.vit.embeddings.patch_embeddings.projection.weight = torch.nn.Parameter(ckpt['patch_embed.proj.weight'])\n",
    "teacher_model.vit.embeddings.patch_embeddings.projection.bias = torch.nn.Parameter(ckpt['patch_embed.proj.bias'])\n",
    "\n",
    "for layer_num in range(24):\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.query.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.weight'][0:1024])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.query.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.bias'][0:1024])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.key.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.weight'][1024:2048])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.key.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.bias'][1024:2048])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.value.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.weight'][2048:])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.value.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.bias'][2048:])\n",
    "    teacher_model.vit.encoder.layer[layer_num].intermediate.dense.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc1.weight'])\n",
    "    teacher_model.vit.encoder.layer[layer_num].intermediate.dense.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc1.bias'])\n",
    "    teacher_model.vit.encoder.layer[layer_num].output.dense.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc2.weight'])\n",
    "    teacher_model.vit.encoder.layer[layer_num].output.dense.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc2.bias'])\n",
    "\n",
    "del ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddaa8a-9602-482c-b58e-404ac542c73e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## train teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994cb229-8184-4f1f-a03b-1b5904a27f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] train loss: 0.67726 valid accuracy: 0.64761\n",
      "Epoch [1] train loss: 0.53478 valid accuracy: 0.79122\n",
      "Epoch [2] train loss: 0.48343 valid accuracy: 0.73138\n",
      "Epoch [3] train loss: 0.44888 valid accuracy: 0.78856\n",
      "Epoch [4] train loss: 0.40197 valid accuracy: 0.78457\n",
      "Epoch [5] train loss: 0.38802 valid accuracy: 0.80319\n",
      "Epoch [6] train loss: 0.36773 valid accuracy: 0.81516\n",
      "Epoch [7] train loss: 0.33496 valid accuracy: 0.81649\n",
      "Epoch [8] train loss: 0.3409 valid accuracy: 0.81782\n",
      "Epoch [9] train loss: 0.29245 valid accuracy: 0.80718\n",
      "Epoch [10] train loss: 0.25785 valid accuracy: 0.82713\n",
      "Epoch [11] train loss: 0.22277 valid accuracy: 0.82846\n",
      "Epoch [12] train loss: 0.19944 valid accuracy: 0.83378\n",
      "Epoch [13] train loss: 0.2243 valid accuracy: 0.83378\n",
      "Epoch [14] train loss: 0.18339 valid accuracy: 0.75266\n",
      "Epoch [15] train loss: 0.1587 valid accuracy: 0.80585\n",
      "Epoch [16] train loss: 0.12743 valid accuracy: 0.82181\n",
      "Epoch [17] train loss: 0.08209 valid accuracy: 0.79654\n",
      "Epoch [18] train loss: 0.07531 end\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/hdd/yuchen/satdata/weights/teacher_model_weights_finetuneweights.pth\"\n",
    "\n",
    "device = args.device\n",
    "teacher_model.to(device)\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=1e-4)\n",
    "\n",
    "best_accur = 0\n",
    "ct_patience = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    \n",
    "    teacher_model.train()\n",
    "    train_loss = 0\n",
    "    ct = 0\n",
    "    for images, labels in train_loader:\n",
    "        ct +=1\n",
    "        print(ct , '/', len(train_loader), end='\\r')\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "        teacher_output = teacher_model(images)\n",
    "        hard_loss = nn.functional.cross_entropy(teacher_output.logits, labels)\n",
    "        train_loss += hard_loss\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        hard_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch}] train loss:', round(train_loss.item()/len(train_loader), 5), end = ' ')\n",
    "\n",
    "    teacher_model.eval()\n",
    "    acc_lst = []\n",
    "    for images, labels in valid_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        teacher_output = teacher_model(images)\n",
    "    \n",
    "        pred = torch.max(teacher_output.logits,axis=1).indices.cpu().numpy()\n",
    "        label = labels.detach().cpu().numpy()\n",
    "        acc_lst.append(len(np.where(pred==label)[0])/len(label))\n",
    "        \n",
    "    curr_accur = np.mean(acc_lst)\n",
    "    if curr_accur>best_accur:\n",
    "        ct_patience = 0\n",
    "        best_accur = curr_accur\n",
    "        torch.save(teacher_model.state_dict(), model_path)\n",
    "    else:\n",
    "        ct_patience += 1\n",
    "        if ct_patience > args.patience:\n",
    "            print('end')\n",
    "            break\n",
    "            \n",
    "    print('valid accuracy:', round(curr_accur, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16a68b-c500-45e8-9ba7-b4ac631ee092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9afd6fbe-54fc-4e55-a10c-e757922c2012",
   "metadata": {},
   "source": [
    "## train student model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c859b8eb-5758-4c71-b864-77f1b23a7e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = \"/hdd/yuchen/satdata/weights/teacher_model_weights.pth\"\n",
    "model_path = \"/hdd/yuchen/satdata/weights/teacher_model_weights_finetuneweights.pth\"\n",
    "teacher_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b5194e-dc59-421a-b97c-678636d1e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "student_model = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224\")\n",
    "student_model.classifier = nn.Linear(in_features=1024, out_features=2, bias=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9cbfdcd-0305-4b64-836b-a434216b47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('/hdd/yuchen/satdata/weights/swinmae100.pth')\n",
    "\n",
    "# student_model.swin.embeddings.patch_embeddings.projection.weight = torch.nn.Parameter(ckpt['patch_embed.proj.weight'])\n",
    "# student_model.swin.embeddings.patch_embeddings.projection.bias = torch.nn.Parameter(ckpt['patch_embed.proj.bias'])\n",
    "# student_model.swin.embeddings.norm.weight = torch.nn.Parameter(ckpt['patch_embed.norm.weight'])\n",
    "# student_model.swin.embeddings.norm.bias = torch.nn.Parameter(ckpt['patch_embed.norm.bias'])\n",
    "\n",
    "# for idx in [0,1,2,3]:\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].layernorm_before.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm1.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].layernorm_before.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm1.bias'])\n",
    "\n",
    "#     curr_size = student_model.swin.encoder.layers[idx].blocks[0].attention.self.query.weight.shape[0]\n",
    "    \n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.self.query.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.weight'][:curr_size])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.self.key.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.weight'][curr_size:curr_size*2])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.self.value.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.weight'][curr_size*2:])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.self.query.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.bias'][:curr_size])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.self.key.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.bias'][curr_size:curr_size*2])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.self.value.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.bias'][curr_size*2:])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.proj.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].attention.output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.proj.bias'])\n",
    "    \n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].layernorm_after.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm2.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].layernorm_after.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm2.bias'])\n",
    "    \n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].intermediate.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc1.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].intermediate.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc1.bias'])\n",
    "    \n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc2.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[0].output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc2.bias'])\n",
    "    \n",
    "    \n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].layernorm_before.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm1.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].layernorm_before.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm1.bias'])\n",
    "    \n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.self.query.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.weight'][:curr_size])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.self.key.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.weight'][curr_size:curr_size*2])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.self.value.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.weight'][curr_size*2:])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.self.query.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.bias'][:curr_size])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.self.key.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.bias'][curr_size:curr_size*2])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.self.value.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.bias'][curr_size*2:])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.proj.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].attention.output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.proj.bias'])\n",
    "    \n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].layernorm_after.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm2.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].layernorm_after.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm2.bias'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].intermediate.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc1.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].intermediate.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc1.bias'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc2.weight'])\n",
    "#     student_model.swin.encoder.layers[idx].blocks[1].output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc2.bias'])\n",
    "    \n",
    "#     if idx != 3:\n",
    "#         student_model.swin.encoder.layers[idx].downsample.reduction.weight =  torch.nn.Parameter(ckpt[f'layers.{idx}.downsample.reduction.weight'])\n",
    "#         student_model.swin.encoder.layers[idx].downsample.norm.weight =  torch.nn.Parameter(ckpt[f'layers.{idx}.downsample.norm.weight'])\n",
    "#         student_model.swin.encoder.layers[idx].downsample.norm.bias =  torch.nn.Parameter(ckpt[f'layers.{idx}.downsample.norm.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62de642b-6845-47da-8101-3b31350f0e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1284dd9c-27d2-4aeb-88d1-56a8d62a572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=3, lambda_param=0.5):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        soft_loss = self.kl_div(\n",
    "            nn.functional.log_softmax(student_logits / self.temperature, dim=1),\n",
    "            nn.functional.softmax(teacher_logits / self.temperature, dim=1),\n",
    "        ) * (self.temperature ** 2)\n",
    "        \n",
    "        hard_loss = nn.functional.cross_entropy(student_logits, labels)\n",
    "        \n",
    "        loss = (1. - self.lambda_param) * hard_loss + self.lambda_param * soft_loss\n",
    "        return  loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3124a1-634c-4917-baf5-ebb6d8b3683f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] train loss: 0.96097 valid accuracy: 0.86436 dist loss: 0.54203\n",
      "Epoch [1] train loss: 0.56693 valid accuracy: 0.87633 dist loss: 0.50666\n",
      "Epoch [2] train loss: 0.37416 valid accuracy: 0.88165 dist loss: 0.58333\n",
      "Epoch [3] train loss: 0.26402 valid accuracy: 0.87234 dist loss: 0.53506\n",
      "Epoch [4] train loss: 0.19509 valid accuracy: 0.86569 dist loss: 0.5148\n",
      "Epoch [5] train loss: 0.14609 valid accuracy: 0.86968 dist loss: 0.46218\n",
      "Epoch [6] train loss: 0.10937 valid accuracy: 0.875 dist loss: 0.47058\n",
      "Epoch [7] train loss: 0.09264 valid accuracy: 0.88165 dist loss: 0.44099\n",
      "Epoch [8] train loss: 0.06385 valid accuracy: 0.87899 dist loss: 0.44042\n",
      "Epoch [9] train loss: 0.05953 valid accuracy: 0.87234 dist loss: 0.44452\n",
      "Epoch [10] train loss: 0.05534 valid accuracy: 0.88298 dist loss: 0.43916\n",
      "Epoch [11] train loss: 0.05059 valid accuracy: 0.85771 dist loss: 0.45308\n",
      "Epoch [12] train loss: 0.05579 valid accuracy: 0.88165 dist loss: 0.42758\n",
      "Epoch [13] train loss: 0.05222 valid accuracy: 0.87766 dist loss: 0.42798\n",
      "Epoch [14] train loss: 0.05123 valid accuracy: 0.87633 dist loss: 0.4738\n",
      "Epoch [15] train loss: 0.04599 valid accuracy: 0.875 dist loss: 0.42403\n",
      "Epoch [16] train loss: 0.0517 valid accuracy: 0.88431 dist loss: 0.45878\n",
      "Epoch [17] train loss: 0.05525 valid accuracy: 0.86569 dist loss: 0.60139\n",
      "Epoch [18] train loss: 0.07473 valid accuracy: 0.87899 dist loss: 0.45571\n",
      "Epoch [19] train loss: 0.05684 valid accuracy: 0.87367 dist loss: 0.46611\n",
      "Epoch [20] train loss: 0.11344 valid accuracy: 0.86303 dist loss: 0.60392\n",
      "Epoch [21] train loss: 0.26616 end\n"
     ]
    }
   ],
   "source": [
    "student_model_path = \"/hdd/yuchen/satdata/weights/student_model_weights_pretrainswin.pth\"\n",
    "args.lambda_param = 0.9\n",
    "\n",
    "# Training settings\n",
    "device = torch.device(args.device)\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=1e-4)\n",
    "distillation_loss = DistillationLoss(temperature = args.temperature, lambda_param = args.lambda_param)\n",
    "\n",
    "best_dist_loss = np.inf\n",
    "patience = args.patience\n",
    "ct_patience = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    student_model.train()\n",
    "    train_loss = 0\n",
    "    ct = 0\n",
    "    for images, labels in train_loader:\n",
    "        ct +=1\n",
    "        print(ct , '/', len(train_loader), end='\\r')\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model(images).logits\n",
    "                        \n",
    "        student_output = student_model(images).logits\n",
    "        loss = distillation_loss(student_output, teacher_output, labels)\n",
    "        train_loss += loss\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch}] train loss:', round(train_loss.item()/len(train_loader), 5), end = ' ')\n",
    "\n",
    "    \n",
    "    teacher_model.eval()\n",
    "    student_model.eval()\n",
    "    \n",
    "    acc_lst = []\n",
    "    val_loss = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        student_output = student_model(images)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model(images)\n",
    "    \n",
    "        pred = torch.max(student_output.logits,axis=1).indices.cpu().numpy()\n",
    "        label = labels.detach().cpu().numpy()\n",
    "        acc_lst.append(len(np.where(pred==label)[0])/len(label))\n",
    "        val_loss += distillation_loss(student_output.logits, teacher_output.logits, labels).item()\n",
    "        \n",
    "    curr_accur = np.mean(acc_lst)\n",
    "    \n",
    "    if val_loss < best_dist_loss:\n",
    "        ct_patience = 0\n",
    "        best_dist_loss = val_loss\n",
    "        student_model_path = f\"/hdd/yuchen/satdata/weights/temp/student_model_weights_{epoch}.pth\"\n",
    "        \n",
    "        torch.save(student_model.state_dict(), student_model_path)\n",
    "    else:\n",
    "        ct_patience+=1\n",
    "        if ct_patience > args.patience:\n",
    "            print('end')\n",
    "            break\n",
    "    print('valid accuracy:', round(curr_accur, 5), 'dist loss:',  round(val_loss/len(valid_loader), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec7229-64a7-495a-a407-ab811edd25ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2222864-fbb4-4bea-975a-2c7c25e0abbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cbfac-5b62-4691-9812-1b17344dbcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645b6165-80fa-4e9e-8679-6531dd3b12c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8634, device='cuda:1', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distillation_loss = DistillationLoss()\n",
    "\n",
    "distillation_loss(student_output, teacher_output, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd28f90a-d243-47f5-932d-06a1c12596cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d03b1f1-086b-40df-a004-2f896460cfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 0, 7, 7, 8, 4, 8, 7, 0, 5, 1, 6, 3, 1, 9, 0, 5, 7, 4, 5, 6, 2, 0,\n",
       "        3, 4, 6, 3, 4, 5, 1, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad573c45-e79d-468a-8f72-0fd514ca08b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a51db-7230-495c-9be3-7634ff1f65bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e5b491-8c08-452e-872d-822439c5b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageDistilTrainer(Trainer):\n",
    "    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.teacher.to(device)\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False):\n",
    "        student_output = self.student(**inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "          teacher_output = self.teacher(**inputs)\n",
    "\n",
    "        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n",
    "\n",
    "        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "\n",
    "        student_target_loss = student_output.loss\n",
    "\n",
    "        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n",
    "        return (loss, student_output) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea506902-9b73-4b0f-b46e-c55a1296e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my-awesome-model\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    logging_dir=f\"{repo_name}/logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repo_name,\n",
    "    )\n",
    "\n",
    "num_labels = len(processed_datasets[\"train\"].features[\"labels\"].names)\n",
    "\n",
    "# initialize models\n",
    "teacher_model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"merve/beans-vit-224\",\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# training MobileNetV2 from scratch\n",
    "student_config = MobileNetV2Config()\n",
    "student_config.num_labels = num_labels\n",
    "student_model = MobileNetV2ForImageClassification(student_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1d069-0d31-42b6-83de-be12da0e4d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ecb828-3a8a-4633-a9fd-266a97f7e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    acc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1204ea-9e20-493a-8f4d-937992ca207a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84381e-3bdf-408c-9d91-f12d5f9395cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "trainer = ImageDistilTrainer(\n",
    "    student_model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    training_args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=teacher_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    temperature=5,\n",
    "    lambda_param=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf316d-54a7-42c5-b7c5-a279b2e0c335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
