{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec1f698-b1e1-4336-9f25-ddb3230b84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTForImageClassification, SwinForImageClassification\n",
    "from transformers import AutoFeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torchvision\n",
    "\n",
    "from functools import partial\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "import sys\n",
    "sys.path.append('/hdd/yuchen/pipeline/training_pipeline')\n",
    "from models.layers import (\n",
    "    Backbone, \n",
    "    PatchEmbed, \n",
    "    Block, \n",
    "    get_abs_pos,\n",
    "    get_norm,\n",
    "    Conv2d,\n",
    "    LastLevelMaxPool\n",
    ")\n",
    "from models.utils import _assert_strides_are_log2_contiguous\n",
    "\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNConvFCHead\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead\n",
    "from torchvision.ops.feature_pyramid_network import LastLevelMaxPool\n",
    "\n",
    "sys.path.append('/hdd/yuchen/pipeline/training_pipeline/models')\n",
    "\n",
    "from faster_rcnn_swin_transformer_detection import transforms\n",
    "from faster_rcnn_swin_transformer_detection.network_files import BackboneWithFPN\n",
    "from faster_rcnn_swin_transformer_detection.swin_transformer import swin_t, Swin_T_Weights, swin_b, Swin_B_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940b05f-76ac-4be8-ad03-f895c64b03f5",
   "metadata": {},
   "source": [
    "## MAE to teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce7b09bb-9a3c-49c2-904d-c4163390bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/hdd/yuchen/satdata/weights/fmow_pretrain.pth')['model']\n",
    "\n",
    "teacher_model.vit.embeddings.patch_embeddings.projection.weight = torch.nn.Parameter(ckpt['patch_embed.proj.weight'])\n",
    "teacher_model.vit.embeddings.patch_embeddings.projection.bias = torch.nn.Parameter(ckpt['patch_embed.proj.bias'])\n",
    "\n",
    "for layer_num in range(24):\n",
    "    \n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.query.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.weight'][0:1024])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.query.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.bias'][0:1024])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.key.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.weight'][1024:2048])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.key.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.bias'][1024:2048])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.value.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.weight'][2048:])\n",
    "    teacher_model.vit.encoder.layer[layer_num].attention.attention.value.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.attn.qkv.bias'][2048:])\n",
    "    \n",
    "    teacher_model.vit.encoder.layer[layer_num].intermediate.dense.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc1.weight'])\n",
    "    teacher_model.vit.encoder.layer[layer_num].intermediate.dense.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc1.bias'])\n",
    "    teacher_model.vit.encoder.layer[layer_num].output.dense.weight = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc2.weight'])\n",
    "    teacher_model.vit.encoder.layer[layer_num].output.dense.bias = torch.nn.Parameter(ckpt[f'blocks.{layer_num}.mlp.fc2.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1d11f47-3d4c-42e1-8b0a-87b63522215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e4088-4328-4995-a050-b937320fc5f8",
   "metadata": {},
   "source": [
    "## MAE to student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b529c76-1bf5-42ff-af9a-bee05fcce283",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/hdd/yuchen/satdata/weights/swinmae100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c97ff0-3ef7-4a16-b2fa-91932b9ef4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "student_model.classifier = nn.Linear(in_features=768, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "124e50a9-bf99-426f-931a-27868bb735cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288, 96])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "ckpt[f'layers.{idx}.blocks.0.attn.qkv.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcb00bbb-2ac8-42e5-a955-0b7937014192",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/hdd/yuchen/satdata/weights/swinmae100.pth')\n",
    "\n",
    "student_model.swin.embeddings.patch_embeddings.projection.weight = torch.nn.Parameter(ckpt['patch_embed.proj.weight'])\n",
    "student_model.swin.embeddings.patch_embeddings.projection.bias = torch.nn.Parameter(ckpt['patch_embed.proj.bias'])\n",
    "student_model.swin.embeddings.norm.weight = torch.nn.Parameter(ckpt['patch_embed.norm.weight'])\n",
    "student_model.swin.embeddings.norm.bias = torch.nn.Parameter(ckpt['patch_embed.norm.bias'])\n",
    "\n",
    "for idx in [0,1,2,3]:\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].layernorm_before.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm1.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].layernorm_before.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm1.bias'])\n",
    "\n",
    "    curr_size = student_model.swin.encoder.layers[idx].blocks[0].attention.self.query.weight.shape[0]\n",
    "    \n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.self.query.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.weight'][:curr_size])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.self.key.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.weight'][curr_size:curr_size*2])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.self.value.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.weight'][curr_size*2:])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.self.query.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.bias'][:curr_size])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.self.key.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.bias'][curr_size:curr_size*2])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.self.value.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.qkv.bias'][curr_size*2:])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.proj.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].attention.output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.attn.proj.bias'])\n",
    "    \n",
    "    student_model.swin.encoder.layers[idx].blocks[0].layernorm_after.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm2.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].layernorm_after.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.norm2.bias'])\n",
    "    \n",
    "    student_model.swin.encoder.layers[idx].blocks[0].intermediate.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc1.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].intermediate.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc1.bias'])\n",
    "    \n",
    "    student_model.swin.encoder.layers[idx].blocks[0].output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc2.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[0].output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.0.mlp.fc2.bias'])\n",
    "    \n",
    "    \n",
    "    student_model.swin.encoder.layers[idx].blocks[1].layernorm_before.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm1.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].layernorm_before.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm1.bias'])\n",
    "    \n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.self.query.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.weight'][:curr_size])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.self.key.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.weight'][curr_size:curr_size*2])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.self.value.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.weight'][curr_size*2:])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.self.query.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.bias'][:curr_size])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.self.key.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.bias'][curr_size:curr_size*2])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.self.value.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.qkv.bias'][curr_size*2:])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.proj.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].attention.output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.attn.proj.bias'])\n",
    "    \n",
    "    student_model.swin.encoder.layers[idx].blocks[1].layernorm_after.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm2.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].layernorm_after.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.norm2.bias'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].intermediate.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc1.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].intermediate.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc1.bias'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].output.dense.weight = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc2.weight'])\n",
    "    student_model.swin.encoder.layers[idx].blocks[1].output.dense.bias = torch.nn.Parameter(ckpt[f'layers.{idx}.blocks.1.mlp.fc2.bias'])\n",
    "    \n",
    "    if idx != 3:\n",
    "        student_model.swin.encoder.layers[idx].downsample.reduction.weight =  torch.nn.Parameter(ckpt[f'layers.{idx}.downsample.reduction.weight'])\n",
    "        student_model.swin.encoder.layers[idx].downsample.norm.weight =  torch.nn.Parameter(ckpt[f'layers.{idx}.downsample.norm.weight'])\n",
    "        student_model.swin.encoder.layers[idx].downsample.norm.bias =  torch.nn.Parameter(ckpt[f'layers.{idx}.downsample.norm.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7089504a-5058-4a44-b9d2-91661c66c210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinForImageClassification(\n",
       "  (swin): SwinModel(\n",
       "    (embeddings): SwinEmbeddings(\n",
       "      (patch_embeddings): SwinPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): SwinEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-5): 6 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0c681-0132-4681-92ec-d60e0ee35134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68faa483-e3de-497a-a1ac-7eb6980ef3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b5d5ab-f8b3-4f95-afda-83c02c1eb1bb",
   "metadata": {},
   "source": [
    "## Student to MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "99d9527b-f521-4f7b-aeff-1abb60fe0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = swin_t(weights=Swin_T_Weights.DEFAULT).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4f26adb-fabd-4886-91d5-8e6cfabf5ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288, 96])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone[1][0].attn.qkv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2f8fd660-fc2d-4a25-9080-b584ba42e8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288, 96])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin = 0\n",
    "torch.nn.Parameter(torch.stack([ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.query.weight'],\n",
    "                                     ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.key.weight'],\n",
    "                                    ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.value.weight']]).flatten(0,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a43f750d-a9b4-4b9a-bd59-31b91e31f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone[0][0].weight = torch.nn.Parameter(ckpt['swin.embeddings.patch_embeddings.projection.weight'])\n",
    "backbone[0][0].bias = torch.nn.Parameter(ckpt['swin.embeddings.patch_embeddings.projection.bias'])\n",
    "backbone[0][2].weight = torch.nn.Parameter(ckpt['swin.embeddings.norm.weight'])\n",
    "backbone[0][2].bias = torch.nn.Parameter(ckpt['swin.embeddings.norm.bias'])\n",
    "\n",
    "for idx in [(1,0),(3,1),(5,2),(7,3)]:\n",
    "    b = idx[0]\n",
    "    swin = idx[1]\n",
    "    backbone[b][0].norm1.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.layernorm_before.weight'])\n",
    "    backbone[b][0].norm1.bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.layernorm_before.bias'])\n",
    "    \n",
    "    backbone[b][0].attn.qkv.weight =  torch.nn.Parameter(torch.stack([ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.query.weight'],\n",
    "                                     ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.key.weight'],\n",
    "                                    ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.value.weight']]).flatten(0,1))\n",
    "    \n",
    "    backbone[b][0].attn.qkv.bias =  torch.nn.Parameter(torch.stack([ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.query.bias'],\n",
    "                                     ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.key.bias'],\n",
    "                                    ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.self.value.bias']]).flatten(0,1))\n",
    "    \n",
    "    backbone[b][0].attn.proj.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.output.dense.weight'])\n",
    "    backbone[b][0].attn.proj.bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.attention.output.dense.bias'])\n",
    "    \n",
    "    backbone[b][0].norm2.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.layernorm_after.weight'])\n",
    "    backbone[b][0].norm2.bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.layernorm_after.bias'])\n",
    "    \n",
    "    backbone[b][0].mlp[0].weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.intermediate.dense.weight'])\n",
    "    backbone[b][0].mlp[0].bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.intermediate.dense.bias'])\n",
    "    backbone[b][0].mlp[3].weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.output.dense.weight'])\n",
    "    backbone[b][0].mlp[3].bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.0.output.dense.bias'])\n",
    "    \n",
    "    backbone[b][1].norm1.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.layernorm_before.weight'])\n",
    "    backbone[b][1].norm1.bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.layernorm_before.bias'])\n",
    "    \n",
    "    backbone[b][1].attn.qkv.weight =  torch.nn.Parameter(torch.stack([ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.self.query.weight'],\n",
    "                                     ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.self.key.weight'],\n",
    "                                    ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.self.value.weight']]).flatten(0,1))\n",
    "    \n",
    "    backbone[b][1].attn.qkv.bias =  torch.nn.Parameter(torch.stack([ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.self.query.bias'],\n",
    "                                     ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.self.key.bias'],\n",
    "                                    ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.self.value.bias']]).flatten(0,1))\n",
    "    \n",
    "    backbone[b][1].attn.proj.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.output.dense.weight'])\n",
    "    backbone[b][1].attn.proj.bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.attention.output.dense.bias'])\n",
    "\n",
    "    backbone[b][1].norm2.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.layernorm_after.weight'])\n",
    "    backbone[b][1].norm2.bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.layernorm_after.bias'])\n",
    "    \n",
    "    backbone[b][1].mlp[0].weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.intermediate.dense.weight'])\n",
    "    backbone[b][1].mlp[0].bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.intermediate.dense.bias'])\n",
    "    backbone[b][1].mlp[3].weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.output.dense.weight'])\n",
    "    backbone[b][1].mlp[3].bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.blocks.1.output.dense.bias'])\n",
    "    \n",
    "    if b != 7:\n",
    "        backbone[b+1].reduction.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.downsample.reduction.weight'])\n",
    "        backbone[b+1].norm.weight = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.downsample.norm.weight'])\n",
    "        backbone[b+1].norm.bias = torch.nn.Parameter(ckpt[f'swin.encoder.layers.{swin}.downsample.norm.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a680c-7be9-4914-83bf-a479ceb6260a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee70db3-7358-4219-83a7-bade85e7d873",
   "metadata": {},
   "source": [
    "## backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33aa1e-8ce3-4879-982e-91237001baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ViT(Backbone):\n",
    "#     \"\"\"\n",
    "#     This module implements Vision Transformer (ViT) backbone in :paper:`vitdet`.\n",
    "#     \"Exploring Plain Vision Transformer Backbones for Object Detection\",\n",
    "#     https://arxiv.org/abs/2203.16527\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         img_size=1024,\n",
    "#         patch_size=16,\n",
    "#         in_chans=3,\n",
    "#         embed_dim=768,\n",
    "#         depth=12,\n",
    "#         num_heads=12,\n",
    "#         mlp_ratio=4.0,\n",
    "#         qkv_bias=True,\n",
    "#         drop_path_rate=0.0,\n",
    "#         norm_layer=nn.LayerNorm,\n",
    "#         act_layer=nn.GELU,\n",
    "#         use_abs_pos=True,\n",
    "#         use_rel_pos=False,\n",
    "#         rel_pos_zero_init=True,\n",
    "#         window_size=0,\n",
    "#         window_block_indexes=(),\n",
    "#         residual_block_indexes=(),\n",
    "#         use_act_checkpoint=False,\n",
    "#         pretrain_img_size=224,\n",
    "#         pretrain_use_cls_token=True,\n",
    "#         out_feature=\"last_feat\",\n",
    "#     ):\n",
    "  \n",
    "#         super().__init__()\n",
    "#         self.pretrain_use_cls_token = pretrain_use_cls_token\n",
    "\n",
    "#         self.patch_embed = PatchEmbed(\n",
    "#             kernel_size=(patch_size, patch_size),\n",
    "#             stride=(patch_size, patch_size),\n",
    "#             in_chans=in_chans,\n",
    "#             embed_dim=embed_dim,\n",
    "#         )\n",
    "\n",
    "#         if use_abs_pos:\n",
    "#             # Initialize absolute positional embedding with pretrain image size.\n",
    "#             num_patches = (pretrain_img_size // patch_size) * (pretrain_img_size // patch_size)\n",
    "#             num_positions = (num_patches + 1) if pretrain_use_cls_token else num_patches\n",
    "#             self.pos_embed = nn.Parameter(torch.zeros(1, num_positions, embed_dim))\n",
    "#         else:\n",
    "#             self.pos_embed = None\n",
    "\n",
    "#         # stochastic depth decay rule\n",
    "#         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "#         self.blocks = nn.ModuleList()\n",
    "#         for i in range(depth):\n",
    "#             block = Block(\n",
    "#                 dim=embed_dim,\n",
    "#                 num_heads=num_heads,\n",
    "#                 mlp_ratio=mlp_ratio,\n",
    "#                 qkv_bias=qkv_bias,\n",
    "#                 drop_path=dpr[i],\n",
    "#                 norm_layer=norm_layer,\n",
    "#                 act_layer=act_layer,\n",
    "#                 use_rel_pos=use_rel_pos,\n",
    "#                 rel_pos_zero_init=rel_pos_zero_init,\n",
    "#                 window_size=window_size if i in window_block_indexes else 0,\n",
    "#                 use_residual_block=i in residual_block_indexes,\n",
    "#                 input_size=(img_size // patch_size, img_size // patch_size),\n",
    "#             )\n",
    "#             if use_act_checkpoint:\n",
    "#                 # TODO: use torch.utils.checkpoint\n",
    "#                 from fairscale.nn.checkpoint import checkpoint_wrapper\n",
    "\n",
    "#                 block = checkpoint_wrapper(block)\n",
    "#             self.blocks.append(block)\n",
    "\n",
    "#         self._out_feature_channels = {out_feature: embed_dim}\n",
    "#         self._out_feature_strides = {out_feature: patch_size}\n",
    "#         self._out_features = [out_feature]\n",
    "\n",
    "#         if self.pos_embed is not None:\n",
    "#             nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "#         self.apply(self._init_weights)\n",
    "\n",
    "#     def _init_weights(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#         elif isinstance(m, nn.LayerNorm):\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "#             nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         if self.pos_embed is not None:\n",
    "#             x = x + get_abs_pos(\n",
    "#                 self.pos_embed, self.pretrain_use_cls_token, (x.shape[1], x.shape[2])\n",
    "#             )\n",
    "\n",
    "#         for blk in self.blocks:\n",
    "#             x = blk(x)\n",
    "\n",
    "#         outputs = {self._out_features[0]: x.permute(0, 3, 1, 2)}\n",
    "#         return outputs\n",
    "\n",
    "      \n",
    "# num_classes = 2\n",
    "# embed_dim, depth, num_heads, dp = 768, 12, 12, 0.1\n",
    "    \n",
    "# embed_dim=1024\n",
    "# depth=24\n",
    "# num_heads=16\n",
    "    \n",
    "# mlp_ratio=4\n",
    "# img_size = 224\n",
    "# patch_size = 16\n",
    "# window_size = 14\n",
    "# mlp_ratio = 4\n",
    "    \n",
    "# # Load the pretrained SqueezeNet1_1 backbone.\n",
    "# net = ViT(  # Single-scale ViT backbone\n",
    "#         img_size=img_size,\n",
    "#         patch_size=patch_size,\n",
    "#         embed_dim=embed_dim,\n",
    "#         depth=depth,\n",
    "#         num_heads=num_heads,\n",
    "#         drop_path_rate=dp,\n",
    "#         window_size=14,\n",
    "#         mlp_ratio=4,\n",
    "#         qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "#         window_block_indexes=[\n",
    "#             # 2, 5, 8 11 for global attention\n",
    "#             0,\n",
    "#             1,\n",
    "#             3,\n",
    "#             4,\n",
    "#             6,\n",
    "#             7,\n",
    "#             9,\n",
    "#             10,\n",
    "#         ],\n",
    "#         residual_block_indexes=[],\n",
    "#         use_rel_pos=True,\n",
    "#         out_feature=\"last_feat\",\n",
    "#     )\n",
    "\n",
    "\n",
    "# # ckpt = torch.utis('weights/mae_pretrain_vit_base.pth')\n",
    "# # ckpt = torch.hub.load_state_dict_from_url('https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth')\n",
    "# # ckpt = torch.hub.load_state_dict_from_url('https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_large.pth')\n",
    "# # ckpt = torch.load('/hdd/yuchen/satdata/weights/fmow_pretrain.pth')\n",
    "\n",
    "# # # from safetensors.torch import save_file, load_file\n",
    "# # # loaded = load_file(\"/hdd/yuchen/satdata/weights/checkpoint-175200/model.safetensors\")\n",
    "# # # net.load_state_dict(loaded, strict=False)\n",
    "# # # ckpt = torch.load('/hdd/yuchen/satdata/weights/scalemae-vitlarge-800.pth')\n",
    "# # net.load_state_dict(ckpt['model'], strict=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
