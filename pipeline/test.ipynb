{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1fcc58-4a8e-4b71-944c-3a1cac6e43be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdd/yuchen/pipeline/training_pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd /hdd/yuchen/pipeline/training_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5395b0dd-f53e-4642-a19d-897ce96e2aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb39fad5-e564-4a3a-bd0b-c84bf8e6b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_configs/custom_data.yaml\n"
     ]
    }
   ],
   "source": [
    "# %%writefile data_configs/custom_data.yaml\n",
    "# # Images and labels direcotry should be relative to train.py\n",
    "# TRAIN_DIR_IMAGES: 'custom_data/train'\n",
    "# TRAIN_DIR_LABELS: 'custom_data/train'\n",
    "# VALID_DIR_IMAGES: 'custom_data/valid'\n",
    "# VALID_DIR_LABELS: 'custom_data/valid'\n",
    "\n",
    "# # Class names.\n",
    "# CLASSES: [\n",
    "#     '__background__',\n",
    "#     'fish', 'jellyfish', 'penguin',\n",
    "#     'shark', 'puffin', 'stingray',\n",
    "#     'starfish'\n",
    "# ]\n",
    "\n",
    "# # Number of classes (object classes + 1 for background class in Faster RCNN).\n",
    "# NC: 8\n",
    "\n",
    "# # Whether to save the predictions of the validation set while training.\n",
    "# SAVE_VALID_PREDICTION_IMAGES: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa3dab86-b4ab-4b63-9dc6-fe305ff64e8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "device cuda:0\n",
      "Checking Labels and images...\n",
      "100%|█████████████████████████████████████| 448/448 [00:00<00:00, 209902.61it/s]\n",
      "Checking Labels and images...\n",
      "100%|█████████████████████████████████████| 127/127 [00:00<00:00, 311142.88it/s]\n",
      "Creating data loaders\n",
      "torch.Size([3, 90, 160])\n",
      "Number of training samples: 448\n",
      "Number of validation samples: 127\n",
      "\n",
      "Building model from scratch...\n",
      "Loading MAE Pretrained ViT Base weights...\n",
      "=============================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape               Output Shape              Param #\n",
      "=============================================================================================================================\n",
      "FasterRCNN (FasterRCNN)                            [1, 3, 160, 160]          [100, 4]                  --\n",
      "├─GeneralizedRCNNTransform (transform)             [1, 3, 160, 160]          [1, 3, 160, 160]          --\n",
      "├─SimpleFeaturePyramid (backbone)                  [1, 3, 160, 160]          [1, 256, 3, 3]            --\n",
      "│    └─ViT (net)                                   [1, 3, 160, 160]          [1, 768, 10, 10]          151,296\n",
      "│    │    └─PatchEmbed (patch_embed)               [1, 3, 160, 160]          [1, 10, 10, 768]          590,592\n",
      "│    │    └─ModuleList (blocks)                    --                        --                        85,147,136\n",
      "│    └─Sequential (simfp_2)                        [1, 768, 10, 10]          [1, 256, 40, 40]          --\n",
      "│    │    └─ConvTranspose2d (0)                    [1, 768, 10, 10]          [1, 384, 20, 20]          1,180,032\n",
      "│    │    └─LayerNorm (1)                          [1, 384, 20, 20]          [1, 384, 20, 20]          768\n",
      "│    │    └─GELU (2)                               [1, 384, 20, 20]          [1, 384, 20, 20]          --\n",
      "│    │    └─ConvTranspose2d (3)                    [1, 384, 20, 20]          [1, 192, 40, 40]          295,104\n",
      "│    │    └─Conv2d (4)                             [1, 192, 40, 40]          [1, 256, 40, 40]          49,664\n",
      "│    │    └─Conv2d (5)                             [1, 256, 40, 40]          [1, 256, 40, 40]          590,336\n",
      "│    └─Sequential (simfp_3)                        [1, 768, 10, 10]          [1, 256, 20, 20]          --\n",
      "│    │    └─ConvTranspose2d (0)                    [1, 768, 10, 10]          [1, 384, 20, 20]          1,180,032\n",
      "│    │    └─Conv2d (1)                             [1, 384, 20, 20]          [1, 256, 20, 20]          98,816\n",
      "│    │    └─Conv2d (2)                             [1, 256, 20, 20]          [1, 256, 20, 20]          590,336\n",
      "│    └─Sequential (simfp_4)                        [1, 768, 10, 10]          [1, 256, 10, 10]          --\n",
      "│    │    └─Conv2d (0)                             [1, 768, 10, 10]          [1, 256, 10, 10]          197,120\n",
      "│    │    └─Conv2d (1)                             [1, 256, 10, 10]          [1, 256, 10, 10]          590,336\n",
      "│    └─Sequential (simfp_5)                        [1, 768, 10, 10]          [1, 256, 5, 5]            --\n",
      "│    │    └─MaxPool2d (0)                          [1, 768, 10, 10]          [1, 768, 5, 5]            --\n",
      "│    │    └─Conv2d (1)                             [1, 768, 5, 5]            [1, 256, 5, 5]            197,120\n",
      "│    │    └─Conv2d (2)                             [1, 256, 5, 5]            [1, 256, 5, 5]            590,336\n",
      "│    └─LastLevelMaxPool (top_block)                [1, 256, 5, 5]            [1, 256, 3, 3]            --\n",
      "├─RegionProposalNetwork (rpn)                      [1, 3, 160, 160]          [1000, 4]                 --\n",
      "│    └─RPNHead (head)                              [1, 256, 40, 40]          [1, 3, 40, 40]            --\n",
      "│    │    └─Sequential (conv)                      [1, 256, 40, 40]          [1, 256, 40, 40]          590,080\n",
      "│    │    └─Conv2d (cls_logits)                    [1, 256, 40, 40]          [1, 3, 40, 40]            771\n",
      "│    │    └─Conv2d (bbox_pred)                     [1, 256, 40, 40]          [1, 12, 40, 40]           3,084\n",
      "│    │    └─Sequential (conv)                      [1, 256, 20, 20]          [1, 256, 20, 20]          (recursive)\n",
      "│    │    └─Conv2d (cls_logits)                    [1, 256, 20, 20]          [1, 3, 20, 20]            (recursive)\n",
      "│    │    └─Conv2d (bbox_pred)                     [1, 256, 20, 20]          [1, 12, 20, 20]           (recursive)\n",
      "│    │    └─Sequential (conv)                      [1, 256, 10, 10]          [1, 256, 10, 10]          (recursive)\n",
      "│    │    └─Conv2d (cls_logits)                    [1, 256, 10, 10]          [1, 3, 10, 10]            (recursive)\n",
      "│    │    └─Conv2d (bbox_pred)                     [1, 256, 10, 10]          [1, 12, 10, 10]           (recursive)\n",
      "│    │    └─Sequential (conv)                      [1, 256, 5, 5]            [1, 256, 5, 5]            (recursive)\n",
      "│    │    └─Conv2d (cls_logits)                    [1, 256, 5, 5]            [1, 3, 5, 5]              (recursive)\n",
      "│    │    └─Conv2d (bbox_pred)                     [1, 256, 5, 5]            [1, 12, 5, 5]             (recursive)\n",
      "│    │    └─Sequential (conv)                      [1, 256, 3, 3]            [1, 256, 3, 3]            (recursive)\n",
      "│    │    └─Conv2d (cls_logits)                    [1, 256, 3, 3]            [1, 3, 3, 3]              (recursive)\n",
      "│    │    └─Conv2d (bbox_pred)                     [1, 256, 3, 3]            [1, 12, 3, 3]             (recursive)\n",
      "│    └─AnchorGenerator (anchor_generator)          [1, 3, 160, 160]          [6402, 4]                 --\n",
      "├─RoIHeads (roi_heads)                             [1, 256, 40, 40]          [100, 4]                  --\n",
      "│    └─MultiScaleRoIAlign (box_roi_pool)           [1, 256, 40, 40]          [1000, 256, 7, 7]         --\n",
      "│    └─TwoMLPHead (box_head)                       [1000, 256, 7, 7]         [1000, 1024]              --\n",
      "│    │    └─Linear (fc6)                           [1000, 12544]             [1000, 1024]              12,846,080\n",
      "│    │    └─Linear (fc7)                           [1000, 1024]              [1000, 1024]              1,049,600\n",
      "│    └─FastRCNNPredictor (box_predictor)           [1000, 1024]              [1000, 8]                 --\n",
      "│    │    └─Linear (cls_score)                     [1000, 1024]              [1000, 8]                 8,200\n",
      "│    │    └─Linear (bbox_pred)                     [1000, 1024]              [1000, 32]                32,800\n",
      "=============================================================================================================================\n",
      "Total params: 105,979,639\n",
      "Trainable params: 105,979,639\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 16.76\n",
      "=============================================================================================================================\n",
      "Input size (MB): 0.31\n",
      "Forward/backward pass size (MB): 136.77\n",
      "Params size (MB): 411.34\n",
      "Estimated Total Size (MB): 548.42\n",
      "=============================================================================================================================\n",
      "105,979,639 total parameters.\n",
      "105,979,639 training parameters.\n",
      "Epoch: [0]  [  0/448]  eta: 0:03:44  lr: 0.000003  loss: 3.0701 (3.0701)  loss_classifier: 2.1328 (2.1328)  loss_box_reg: 0.0168 (0.0168)  loss_objectness: 0.6931 (0.6931)  loss_rpn_box_reg: 0.2274 (0.2274)  time: 0.5013  data: 0.0936  max mem: 1234\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/train.py\", line 575, in <module>\n",
      "    main(args)\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/train.py\", line 415, in main\n",
      "    batch_loss_rpn_list = train_one_epoch(\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/torch_utils/engine.py\", line 52, in train_one_epoch\n",
      "    loss_dict = model(images, targets)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 104, in forward\n",
      "    proposals, proposal_losses = self.rpn(images, features, targets)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torchvision/models/detection/rpn.py\", line 361, in forward\n",
      "    anchors = self.anchor_generator(images, features)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py\", line 127, in forward\n",
      "    anchors_over_all_feature_maps = self.grid_anchors(grid_sizes, strides)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torchvision/models/detection/anchor_utils.py\", line 103, in grid_anchors\n",
      "    shifts_y = torch.arange(0, grid_height, dtype=torch.int32, device=device) * stride_height\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --data data_configs/custom_data.yaml --epochs 5 --imgsz 160 --device cuda:0 --model fasterrcnn_vitdet --name custom_training --batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb87e0-773d-42b1-9563-dc8ee801a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "312b6866-3475-478b-8a29-4ef2e0f2b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building from model name arguments...\n",
      "Loading MAE Pretrained ViT Base weights...\n",
      "Test instances: 63\n",
      "(160, 120, 3)\n",
      "Image 1 done...\n",
      "--------------------------------------------------\n",
      "(120, 160, 3)\n",
      "Image 2 done...\n",
      "--------------------------------------------------\n",
      "(120, 160, 3)\n",
      "Image 3 done...\n",
      "--------------------------------------------------\n",
      "(160, 120, 3)\n",
      "Image 4 done...\n",
      "--------------------------------------------------\n",
      "(160, 90, 3)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/inference.py\", line 244, in <module>\n",
      "    main(args)\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/inference.py\", line 199, in main\n",
      "    outputs = model(image.to(DEVICE))\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 101, in forward\n",
      "    features = self.backbone(images.tensors)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/models/fasterrcnn_vitdet.py\", line 278, in forward\n",
      "    bottom_up_features = self.net(x)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/models/fasterrcnn_vitdet.py\", line 150, in forward\n",
      "    x = blk(x)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/models/layers.py\", line 786, in forward\n",
      "    x = self.attn(x)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/hdd/yuchen/pipeline/training_pipeline/models/layers.py\", line 298, in forward\n",
      "    attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
      "  File \"/hdd/yuchen/anaconda3/envs/291k/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1675, in __getattr__\n",
      "    def __getattr__(self, name: str) -> Any:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --input /hdd/yuchen/pipeline/training_pipeline/custom_data/test --data data_configs/custom_data.yaml --weights outputs/training/custom_training/best_model.pth --imgsz 160 --device cuda:0 --model fasterrcnn_vitdet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6b4af-dd8f-4265-a635-2807984bf65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f321b0-53de-49f2-812b-df40275afde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1bcc6-d457-4d64-95e0-c64d2fbea937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dc02e-1d2e-478d-b29e-1439d39ff35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17060e5c-0193-488c-8d92-fc7a630cb8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
